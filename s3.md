# Amazon S3

Infinitely scaling storage

## Buckets
* You can store objects (files) in buckets (directories)
* Buckets must have a globally unique name
* Buckets are defined at the region level
* Naming convention:
  * No uppercase
  * No underscore
  * 3-63 characters long
  * Not an IP
  * must start with lowercase letter or number

## Objects
* Objects (files) have a key
* key is a FULL path (without bucket name). Example: s3://my-bucket/**photos/summer/cat1.png**
* The name is just last segment. Example: cat1.png
* Max Object size is 5TB
* Max upload: 5GB. If uploading more than 5GB then must use "multipart upload"
* Object can contain metadata, tags and versioning id (if enabled versioning)

## Versioning
* It must be enabled at bucket level
* Any file not versioned prior to enabling versioning will have version null
* Suspending versioning does not delete previous versions

## Encryption
* There are 4 methods of encrypting objects in S3
  * SSE-S3: encrypts S3 objects using keys handled & managed by AWS
  * SSE-KMS: leverage AWS Key Management Service to manage encryption keys
  * SSE-C: when you want to manage your encryption keys
  * Client-Side Encryption

### SSE-S3
* Encryption using keys handled & managed by Amazon S3
* Object is encrypted server side
* AES-256 encryption type
* Must set header: "x-amz-server-side-encryption": "AES256"

### SSE-KMS
* Encryption using keys handled & managed by KMS
* Object is encrypted server side
* Must set header: "x-amz-server-side-encryption": "aws:kms"
* KMS Advantage: user control + audit trail

### SSE-C
* Encryption using keys handled & managed by user (outside AWS)
* Object is encrypted server side
* You must provide Encryption key in HTTP headers so HTTPS must be used
* Amazon S3 does not store encryption key you provide. After encryption key is deleted
* Therefore you must provide encryption key every time you want to open that file
* Customer Advantage: user control + audit trail
* Customer Disadvantage: managing keys on user end

### Client Side Encryption
* Encryption happens on Client side before sending files to AWS S3. 
* Using keys handled & managed by user (outside AWS)
* Client must decrypt data themselves when retrieving from S3
* Customer Advantage: user fully control

### Encryption in transit (SSL/TLS)
* Amazon exposes both: HTTP (data in transit not encrypted) and HTTPS (data in transit encrypted) endpoints
* You can choose any of them. HTTPS is recommended.
* HTTPS is mandatory for SSE-C

## S3 Security
Security can be:
* User based: IAM policies - which API calls should be allowed for specific user from IAM console
* Resource based:
  * Bucket policies: bucket wide rules from the S3 console - allows cross account.
  * Object Access Control List (ACL) - finer grain
  * Bucket Access Control List (ACL)
* IAM role or user can access an S3 Object if (both apply):
  * user IAM permission allows it OR resource policy ALLOWS it
  * AND there is no explicit DENY

### S3 Bucket policies
* JSON based policies that define:
  * Resource (bucket or object) that we will grant/deny access to
  * Action: Set of API action to allow or deny (getObject, uploadObject etc...)
  * Effect: Allow/Deny
  * Principal: IAM role or user to apply the policy to.
* Use S3 bucket policy to:
  * Grant public access to the bucket
  * Force objects to be encrypted at upload
  * Grant access to another account (cross account)
* There is a way to block public access to the bucket through:
  * ACL
  * Policies

### S3 Security - Other
* Networking - support access from VPC (for instances in VPC without www internet)
* Logging and audit - S3 Access logs can be stored in other S3 bucket, API Calls can be logged in AWS Cloud Trail
* User Security: Multi Factor Authentication (MFA) - S3 supports for MFA Delete
* Pre Signed URL's - URL's that are valid only for a limited time (ex: premium video service for logged in users)

### S3 - Eventual Consistency Model
* Read after write consistency for PUT of new objects
  * That means you will always get correct version of the file when you upload new file
  * Except: GET 404 -> PUT 200 -> GET 404. So if you ask if object exists and it doesn't then you upload that new file and ask if it exists again you may get 404 again.
* Eventual consistency (old version of a file for a very short time after doing some CRUD for that file) for DELET's and PUT's of existing objects:
  * PUT 200 (a) -> PUT 200 (a') -> GET 200 (may return a instead of a')
  * DELETE 200 -> GET 200 (may return a file after delete)
* There is no way to enable strong consistency